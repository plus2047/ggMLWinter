{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset combined: 3731 (train=2000/validate=1000/test=731)\n",
      "model_params.max_seq_len set to 68.\n",
      "INFO:tensorflow:Model using LuongMonotonicAttention.\n",
      "INFO:tensorflow:Model using LuongMonotonicAttention.\n",
      "ERROR:tensorflow:Error: Input value Tensor(\"Decoder_1/dynamic_decode/while/BasicDecoderStep/CustomHelperSample/zeros:0\", shape=(1,), dtype=float32) has dtype <dtype: 'float32'>, but expected dtype <dtype: 'int32'>.  This leads to undefined behavior and will be an error in future versions of TensorFlow.  Traceback:\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/ipykernel_launcher.py\", line 16, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/traitlets/config/application.py\", line 664, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/ipykernel/kernelapp.py\", line 563, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tornado/platform/asyncio.py\", line 148, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/asyncio/base_events.py\", line 534, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/asyncio/base_events.py\", line 1771, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/asyncio/events.py\", line 88, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 690, in <lambda>\n",
      "    lambda f: self._run_callback(functools.partial(callback, future))\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tornado/ioloop.py\", line 743, in _run_callback\n",
      "    ret = callback()\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tornado/gen.py\", line 787, in inner\n",
      "    self.run()\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tornado/gen.py\", line 748, in run\n",
      "    yielded = self.gen.send(value)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 361, in process_one\n",
      "    yield gen.maybe_future(dispatch(*args))\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 268, in dispatch_shell\n",
      "    yield gen.maybe_future(handler(stream, idents, msg))\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/ipykernel/kernelbase.py\", line 541, in execute_request\n",
      "    user_expressions, allow_stdin,\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tornado/gen.py\", line 209, in wrapper\n",
      "    yielded = next(result)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/ipykernel/ipkernel.py\", line 300, in do_execute\n",
      "    res = shell.run_cell(code, store_history=store_history, silent=silent)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/ipykernel/zmqshell.py\", line 536, in run_cell\n",
      "    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2848, in run_cell\n",
      "    raw_cell, store_history, silent, shell_futures)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 2874, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/IPython/core/async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3051, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3242, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-137-782f5d6ea94a>\", line 25, in <module>\n",
      "    eval_model = model.FontRNN(eval_hps_model, reuse=True)\n",
      "  File \"/Users/plus/workspace/ggMLWinter/FontRNN/code/model.py\", line 71, in __init__\n",
      "    self.build_model(reuse=reuse)\n",
      "  File \"/Users/plus/workspace/ggMLWinter/FontRNN/code/model.py\", line 263, in build_model\n",
      "    dec_out, timemajor_attn_hist = self.decoder(enc_last_h, enc_all_h, self.enc_seq_lens, dec_input)\n",
      "  File \"/Users/plus/workspace/ggMLWinter/FontRNN/code/model.py\", line 231, in decoder\n",
      "    scope='dynamic_decode')\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tensorflow_core/contrib/seq2seq/python/ops/decoder.py\", line 469, in dynamic_decode\n",
      "    swap_memory=swap_memory)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2753, in while_loop\n",
      "    return_same_structure)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2245, in BuildLoop\n",
      "    pred, body, original_loop_vars, loop_vars, shape_invariants)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/control_flow_ops.py\", line 2170, in _BuildLoop\n",
      "    body_result = body(*packed_vars_for_body)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tensorflow_core/contrib/seq2seq/python/ops/decoder.py\", line 452, in body\n",
      "    outputs_ta, emit)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\", line 536, in map_structure\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/util/nest.py\", line 536, in <listcomp>\n",
      "    structure[0], [func(*x) for x in entries],\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tensorflow_core/contrib/seq2seq/python/ops/decoder.py\", line 451, in <lambda>\n",
      "    outputs_ta = nest.map_structure(lambda ta, out: ta.write(time, out),\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 1084, in write\n",
      "    return self._implementation.write(index, value, name=name)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/util/tf_should_use.py\", line 198, in wrapped\n",
      "    return _add_should_use_warning(fn(*args, **kwargs))\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 268, in write\n",
      "    _check_dtypes(value, self._dtype)\n",
      "  File \"/Users/plus/.bin/miniconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/tensor_array_ops.py\", line 1239, in _check_dtypes\n",
      "    \"\".join(traceback.format_stack())))\n",
      "\n",
      "Loading model ../log/SXmodel1030-1013/vector-8200.\n",
      "INFO:tensorflow:Restoring parameters from ../log/SXmodel1030-1013/vector-8200\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "# from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import train\n",
    "import model\n",
    "import utils\n",
    "import gmm\n",
    "\n",
    "data_dir =  '../data'\n",
    "model_dir = '../log/SXmodel1030-1013'\n",
    "\n",
    "# load dataset and paramters\n",
    "[train_set, valid_set, test_set, std_train_set, std_valid_set, std_test_set, \n",
    "     hps_model, eval_hps_model] = train.load_env(data_dir, model_dir)\n",
    "\n",
    "train_size, valid_size, test_size = 2000, 1000, 731\n",
    "\n",
    "# construct model:\n",
    "train.reset_graph()\n",
    "train_model = model.FontRNN(hps_model)\n",
    "eval_model = model.FontRNN(eval_hps_model, reuse=True)\n",
    "\n",
    "# load trained checkpoint\n",
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "train.load_checkpoint(sess, model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(sess, testmodel, input_stroke):\n",
    "    stroke_len = len(input_stroke)\n",
    "    input_stroke = utils.to_big_strokes(input_stroke, max_len=testmodel.hps.max_seq_len).tolist()\n",
    "    input_stroke.insert(0, [0, 0, 1, 0, 0])\n",
    "    feed = {testmodel.enc_input_data: [input_stroke],\n",
    "            testmodel.enc_seq_lens: [stroke_len],\n",
    "            }\n",
    "    output = sess.run([testmodel.pi, testmodel.mu1, testmodel.mu2, testmodel.sigma1,\n",
    "                       testmodel.sigma2, testmodel.corr, testmodel.pen,\n",
    "                       testmodel.timemajor_alignment_history],\n",
    "                      feed)\n",
    "    gmm_params = output[:-1]\n",
    "    timemajor_alignment_history = output[7]\n",
    "\n",
    "    return gmm_params, timemajor_alignment_history\n",
    "\n",
    "\n",
    "def sample_from_params(params, temp=0.1, greedy=False):\n",
    "    [o_pi, o_mu1, o_mu2, o_sigma1, o_sigma2, o_corr, o_pen] = params\n",
    "\n",
    "    max_len = o_pi.shape[0]\n",
    "    num_mixture = o_pi.shape[1]\n",
    "\n",
    "    strokes = np.zeros((max_len, 5), dtype=np.float32)\n",
    "\n",
    "    for step in range(max_len):\n",
    "        next_x1 = 0\n",
    "        next_x2 = 0\n",
    "        eos = [0, 0, 0]\n",
    "        eos[np.argmax(o_pen[step])] = 1\n",
    "        for mixture in range(num_mixture):\n",
    "            x1, x2 = gmm.sample_gaussian_2d(o_mu1[step][mixture], o_mu2[step][mixture],\n",
    "                                            o_sigma1[step][mixture], o_sigma2[step][mixture],\n",
    "                                            o_corr[step][mixture], np.sqrt(temp), greedy)\n",
    "            next_x1 += x1 * o_pi[step][mixture]\n",
    "            next_x2 += x2 * o_pi[step][mixture]\n",
    "        strokes[step, :] = [next_x1, next_x2, eos[0], eos[1], eos[2]]\n",
    "    strokes = utils.to_normal_strokes(strokes)\n",
    "    return strokes\n",
    "\n",
    "def to_absolute_coordinate(mat, scale_factor=300):\n",
    "    low_tri_matrix = np.tril(np.ones((mat.shape[0], mat.shape[0])), 0)\n",
    "    mat[:, :2] = np.rint(scale_factor * np.matmul(low_tri_matrix, mat[:, :2]))\n",
    "    return mat\n",
    "\n",
    "def sub_draw(mat, count_lim):\n",
    "    cnt = pre_i = 0\n",
    "    xmax, xmin = np.max(mat[:, 0]), np.min(mat[:, 0])\n",
    "    ymax, ymin = np.max(mat[:, 1]), np.min(mat[:, 1])\n",
    "    for i in range(mat.shape[0]):\n",
    "        if mat[i][2] == 1:\n",
    "            plt.plot(\n",
    "                mat[pre_i:i + 1, 0],\n",
    "                mat[pre_i:i + 1, 1],\n",
    "                linewidth=3)\n",
    "            cnt += 1\n",
    "            pre_i = i + 1\n",
    "            if cnt >= count_lim:\n",
    "                break\n",
    "    # plt.axis('off')\n",
    "    # plt.hlines(ylim / 2, 0, xlim)\n",
    "    # plt.vlines(xlim / 2, 0, ylim)\n",
    "    plt.xlim((xmin - 10, xmax + 10))\n",
    "    plt.ylim((ymin - 10, ymax + 10))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.gca().invert_yaxis()\n",
    "    # plt.title(\"Groud Truth\")\n",
    "\n",
    "def draw_grid(mlist, draw_count_lim, width=16):\n",
    "    def to_abs_list(mat_list):\n",
    "        return [to_absolute_coordinate(mat.copy()) for mat in mat_list]\n",
    "\n",
    "    mlist = to_abs_list(mlist)\n",
    "    count = len(mlist)\n",
    "    lines = (count - 1) // width + 1\n",
    "    \n",
    "    plt.figure(figsize=(3 * width, 3 * lines))\n",
    "    for i in range(count):\n",
    "        plt.subplot(lines, width, 1 + i)\n",
    "        sub_draw(mlist[i], draw_count_lim)\n",
    "\n",
    "def index2mat(index):\n",
    "    if index < train_size:\n",
    "        std_dataset = std_train_set\n",
    "        dataset = train_set\n",
    "    elif index < train_size + valid_size:\n",
    "        std_dataset = std_valid_set\n",
    "        dataset = valid_set\n",
    "        index -= train_size\n",
    "    else:\n",
    "        std_dataset = std_test_set\n",
    "        dataset = test_set\n",
    "        index -= train_size + valid_size\n",
    "\n",
    "    from_strokes = np.copy(std_dataset.strokes[index])\n",
    "    to_strokes = np.copy(dataset.strokes[index])\n",
    "    params, timemajor_alignment_history = test_model(sess, eval_model, from_strokes)\n",
    "    generated_strokes = sample_from_params(params, greedy=True)\n",
    "\n",
    "    return from_strokes, to_strokes, generated_strokes\n",
    "\n",
    "def draw_by_index_list(index_list, width=16):\n",
    "    mlist = []\n",
    "    for index in index_list:\n",
    "        from_strokes, to_strokes, generated_strokes = index2mat(index)\n",
    "        mlist.append(generated_strokes)\n",
    "    draw_grid(mlist, 10000, width)\n",
    "\n",
    "data = np.load(\"../data/1030-1013-tag-only.npz\")\n",
    "tag_list = data[\"tag\"]\n",
    "word2idx = {w : i for i, w in enumerate(tag_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for w, idx in word2idx.items():\n",
    "    draw_by_index_list([idx], 1)\n",
    "    plt.savefig(\"../../web/static/font/%s.png\" % ord(w))\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
